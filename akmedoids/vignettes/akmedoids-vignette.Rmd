---
title: "`Akmedoids` R-package for longitudinal dataset: A guide to measuring long-term inequality in the exposure to crime at small area level"
author: |
  | Adepeju, M., Langton, S., and Bannister, J.
  | Big Data Centre, Manchester Metropolitan University, Manchester, M15 6BH
date: "`r Sys.Date()`"
output:
  #word_document: default
  #always_allow_html: yes
  pdf_document: default
always_allow_html: yes
fig_caption: yes
bibliography: references.bib
abstract: The `akmedoids` advances a set of R-functions for longitudinal clustering
  of trajectories based on the similarities of their long-term trends and determines
  the optimal solution based on the Calinski-Harabatz criterion (Calinski and Harabatz,
  1974). The package also include a number of other useful functions for exploring
  and manipulating longitudinal data prior to the clustering process.
  The primary goal of the `akmedoids` package is to aid replication of crime inequality study under crime drop for cities around the world (see Adepeju et al. 2019). In this document, we provide a guide to carrying out this investigation. Meanwhile, the package can also be employed for other similar longitudinal studies. 
vignette: |
  %\VignetteIndexEntry{Vignette Title} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---

<style type="text/css">

h1.title {
  font-size: 16px;
  color: Black;
  text-align: center;
}

h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 18px;
  font-family: "Arial", Times, serif;
  color: Black;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 12px;
  font-family: "Arial", Times, serif;
  color: Black;
  text-align: center;
}

h4.abstract { /* Header 4 - and the author and data headers use this too  */
  font-size: 12px;
  font-family: "Arial", Times, serif;
  color: black;
  text-align: center;
}

h4.affiliation{ /* Header 4 - and the author and data headers use this too  */
  font-size: 12px;
  font-family: "Arial", Times, serif;
  color: black;
  text-align: center;
}

body, td {
   font-size: 11px;
}
code.r{
  font-size: 10px;
}
pre {
  font-size: 11px
}
h1 { /* Header 1 */
  font-size: 14px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 12px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 11px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;

</style>

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
# Introduction
Longitudinal clustering analysis has been widely used in the social and behavioral sciences for understanding the developmental processes of a subject over time. Examples of this type of application include developmental

In other fields, choosing initialisation points has been shown to optimize the final cluster solution and provide greater computational efficiency (Bradley and Fayyad, 1998). Moreover, no attempt has been made to tailor the iteration procedure to minimise the sensitivity of k-means to short-term fluctuations. This presents us with room for improvement in developing a bespoke implementation of k-means clustering for use in longitudinal crime concentration research. 

.

The goal of this document is to provide a description of the functions contained in this package and demonstrate how they can be used in a real application. Examples provided throughout this document should provide a template for the uptake of these package in the longitudinal studies in other fields. These functions are divided into two categories, `data manipulation` and `data clustering` functions. Details are as follow:

#1. Data manipulation
Table \ref{tab:table1} shows the key functions under this category and their descriptions. This include functions to replace certain entries, such as missing data and outliers, in the data. Data manipulation functions also also include functions to carry out basic conversion of data from one type of measure to another. For example, from `counts` to `rates`. In order to demonstrate the utility of these functions, we use a simulated dataset `traj` which is provided with the `akmediods` package. This dataset can be accessed by typing `traj` in R console after calling the library and can also be found in the package folder under `".../akmedoids/data/data/traj.rda"`. 

```{r, echo=FALSE, include=FALSE}
require(knitr)
library(flextable)
library(kableExtra)
col1 <- c("1", "2","3","4", "5")
col2 <- c("`dataImputation`","`rates`", "`outlierDetect`","`wSpaces`", "`props`")
col3 <- c("Data imputation for longitudinal data", "Conversion of counts to rates", "Outlier detection and replacement","Whitespaces removal", "Conversion of counts (or rates) to 'Proportion'")
col4 <- c("Calculates any missing entries (`NA`, `Inf`, `null`) in a longitudinal data, according to a specified method","Calculates rates from 'observed' count and a denominator data", "Identifies outlier observations in the data, and replace or remove them","Removes all the leading and trailing whitespaces in a longitudinal data","Converts counts or rates observation to 'proportion'")
tble <- data.frame(col1, col2, col3, col4)
tble <- tble
```

```{r table1, results='asis', echo=FALSE, tidy.opts=list(width.cutoff=50)}
knitr::kable(tble, caption = "`Data manipulation` functions", col.names = c("SN","Function","Title","Description")) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "8em", background = "white") %>%
  column_spec(3, width = "12em", background = "white") %>%
  column_spec(4, width = "16em", background = "white")#%>%
  #row_spec(3:5, bold = T, color = "white", background = "#D7261E")
```

### (i) `"dataImputation"` functions
This function calculates any missing entries (such as `NA`, `Inf`, `null`) in a data, according to a specified method. The `traj` dataset contains a number of missing values which might have not been observed (such as in the case of `NA`) or have resulted due to a calculation error (such as in the case of `Inf`). The `dataImputation` function can be used to replace these entries based on the values of other cells. Two methods are proposed. First, an `arithmetic` method which uses the `mean`, `minimum` or `maximum` value from the corresponding rows or columns in which the missing values are located. A user decides what option is most appropriate for the data at hand. Second, a `regression` method which uses a linear regression line to estimate the missing values on a line. are found and then estimates their values. Note that only the missing data points derive values from the regression line while the rest of the data points retain their original values. The function terminates if there are trajectories with only one observation. Below is a demonstration of how the `regression` option will estimate the missing values of the `traj` dataset. 

```{r, eval=FALSE}
#installing packages
install.packages("devtools")
devtools::install_github("manalytics/packages/akmedoids")
```

```{r, eval=TRUE}
#loading the package
library(akmedoids)
```

```{r, eval=TRUE}
#view the first few rows
head(traj)
nrow(traj) #no. of rows
ncol(traj) #no. of columns
```

The first column of `traj` is a `id` (unique) field. In many applications, it is essential to preserve this field in order to allow mapping of the result to other datasets, such as the spatial location datasets (`.shp`). In most of the functions in this package, we added an option that allow users to either retain or ignore the unique field. Below is the results of the application of the `dataImputation` function to `traj` dataset.

```{r, eval=TRUE}
imp_traj <- dataImputation(traj, id_field = TRUE, method = 2, 
               replace_with = 1, fill_zeros = FALSE)
head(imp_traj)
```

From the syntax above, the argument `method = 2` refers to the regression technique, while the argument `replace_with = 1` indicate `linear` option (which is currently the only available option under this method). Figure \ref{fig:figs1} is a graphical illustration of how the technique derives the missing values.


```{r figs1, echo=FALSE, fig.width=6,fig.height=7,fig.align="center", fig.cap="\\label{fig:figs1} data imputation with regression"}

par(mar=c(2,2,2,2)+0.1)
par(adj = 0)
par(mfrow=c(6,2))
dat <- as.data.frame(traj)
t_name <- as.vector(traj[,1])
dat <- dat[,2:ncol(dat)]
#if(k==nrow(dat)){
  #}
#head(dat)
for(k in 1:nrow(dat)){ #k<-2
  y <- suppressWarnings(as.numeric(as.character(dat[k,])))
  x <- 1:length(y)
  known <- data.frame(x, y)
  known_1 <- data.frame(known[is.na(known[,2])|is.infinite(known[,2]),])  #
  known_2 <- data.frame(known[!is.na(known[,2])&!is.infinite(known[,2]),])
  #train the available data using linear regression
  model.lm <- lm(y ~ x, data = known_2)
  # Use predict the y value for the removed data
  newY <- predict(model.lm, newdata = data.frame(x = known_1[,1]))
   l_pred <- predict(model.lm, newdata = data.frame(1:9)) #line
  #add to the original data.
  dat[k, known_1[,1]] <- newY
  #Add the predicted points to the original data
  #dev.new()
  #plot(1:10, col=2)
  plot (known$x, known$y, type="o", main=paste("traj_id:",t_name[k], sep=" "), font.main = 1)
  if(!length(newY)==0){#plot only if it has elements
  lines(l_pred, lty="dotted", col="red", lwd=2)
  }
  points(known_1[,1], newY, col = "red")
}
#point legend
plot_colors <- c("black","red")
text <- c("Observed points", "Predicted points")
plot.new()
par(xpd=TRUE)
legend("center",legend = text, text.width = max(sapply(text, strwidth)),
       col=plot_colors, pch = 1, cex=1, horiz = FALSE)
par(xpd=FALSE)

#line legend
plot_colors <- c("black","red")
text <- c("line joining observed points", "regression line predicting missing points")
plot.new()
par(xpd=TRUE)
legend("center",legend = text, text.width = max(sapply(text, strwidth)),
       col=plot_colors, lwd=1, cex=1, lty=c(1,2), horiz = FALSE)
par(xpd=FALSE)
```


### Special application of '`dataImputation`' function:

Apart from census years, it is generally difficult to obtain denominator information (i.e. population) for local geographical units. In a longitudinal studies, this challenge pose a significant drawback to accurate estimation of measures such as crime risk level at local areas across a space. However, we can interpolate and/or extrapolate the missing population information given a limited data points. The `dataImputation` function can be used for this purpose.

The key step to achieving this task is to create a matrix (`.csv`), containing both the available and the missing fields, in which `NA` is entered for any missing cell. See below an example of a population data (with only two fields) and its corresponding `input` matrix for the `dataImputation` function. This example is design to conform with the `traj` dataset above. 

```{r, eval=TRUE}
#view the data first few rows
head(population)
nrow(population) #no. of rows
ncol(population) #no. of columns
```

The corresponding input dataset, named `population2` is prepared as shown below:

```{r, echo=FALSE}
#create a matrix of the same rows and column as the `traj` data
pop <- as.data.frame(matrix(0, nrow(population), ncol(traj)))
colnames(pop) <- names(traj) 
pop[,1] <- as.vector(as.character(population[,1]))
pop[,4] <- as.vector(as.character(population[,2]))
pop[,8] <- as.vector(as.character(population[,3]))
list_ <- c(2, 3, 5, 6, 7, 9, 10)
for(u_ in 1:length(list_)){ #u_<-1
  pop[,list_[u_]] <- "NA"
}
#transfer the `location_id`
#c(population$location_id, names(traj)[2:ncol(traj)])
#colnames(pop) <- c(population$location_id[1], names(traj)[2:ncol(traj)])
#pop$location_id <- populaton$location_ids
head(pop)
population2 <- pop
```

The `dataImputation` function is ran as follows:

```{r, eval=TRUE}
#the
pop_imp_result <- dataImputation(population2, id_field = TRUE, method = 2, 
               replace_with = 1, fill_zeros = FALSE)
head(pop_imp_result)
```

The above is the result of the `imputation` process which in this case is synonymous to fitting a straight to the two data points and then estimate the population of other fields. More advance options will be provided in the future. 


###(ii) `"rates"` function

Given a longitudinal matrix ($m\times n$) containing some count data and a corresponding denominator information with the same number of columns ($n$), the `rate` function derives the 'rates' measures (e.g. count per 100 people) for the `id` ***rows that match***. We demonstrate this with the `imp_traj` data and the population estimates ('`pop_imp_result`') derived above. 

```{r, eval=TRUE}
#Example of estimation of crimes per 200 residents
crime_per_200_people <- rates(imp_traj, denomin=pop_imp_result, id_field=TRUE, 
                              multiplier = 200)
#view the result
crime_per_200_people

#check the number of rows
nrow(crime_per_200_people)

```

It can be observed that the number of rows in the output is 9. This indicates that only 9 `location_ids` in the `count` and `population` data match.  

###(iii) `"props"` function {#props}

Given longitudinal data, `props` function convert each observation (entry in each cell) to the proportion of the sum of each column. In other words, each observation is divided by the sum of the column where it is located. i.e. `prop = [a cell value] / sum[corresponding column]`. This 'proportion' measure was applied by [@Adepeju2019] in their study focusing on measuring the long-term inequality in the exposure to crime at small geographical areas. Using the 'rates' estimates obtained above:  


```{r, eval=TRUE}
#Proportions of crimes per 200 residents
prop_crime_per200_people <- props(crime_per_200_people, id_field = TRUE)

prop_crime_per200_people
```

**Note**: In many cases, calculating rates do results into a few `Inf` and `NA` cell entries. For example, in any cell where the 'population' data is missing or contains `character` inputs, the `rates` formular returns `Inf` or `NA`. We recommend that users re-run the `dataImputation` function after running `rates` function in order to address such occurences.

### (iv) `"outlierDetect"` function
This function is aimed at allowing users to identify any outlier observations in a longitudinal data, and replace or remove them accordingly. The first step to dealing with outliers is to first visualise (plot) the data. After that, a user can then decide the cut-off for the outliers. The `outlierDetect` function provides two options for setting a cut-off value: (i) `quantile` method, that is any observation over certain quantile of the value distribution, and (ii) `manual` method, in which a user defines the cut-off value. The 'replacement' option for any detected outliers include either to use the mean value of the row or the mean value of the column in which the outlier entry is found. The user also has the option to simply remove the trajectory that contains the outlier. In deciding whether a trajectory contains outlier or not, the `count` argument allows the user to decide the number of observations (in a trajectory) that must exceed the cut-off. Using the `imp_traj` data, the `outlierDetect` function will work as follows:

```{r figs2, echo=TRUE, fig.width=8,fig.height=4,fig.align="center", fig.cap="\\label{fig:figs2}, Identifying outliers"}

#Plotting the data using ggplot library
library(ggplot2)
library(reshape2)

#converting the wide data format into stacked format for plotting
imp_traj_long <- melt(imp_traj, id="location_ids") 
head(imp_traj_long)#previewing the first few rows

#plot function
p <-  ggplot(imp_traj_long, aes(x=variable, y=value,
            group=location_ids, color=location_ids)) + 
            geom_point() + 
            geom_line()
print(p)
```
The above plot function generates the plot shown in Figure \ref{fig:figs2}

Based on Figure \ref{fig:figs2}, if we assume that observations of `x2001`, `x2007` and `x2008` of trajectory with id `E01004806` are outliers, we can set the cut-off (`threshold`) value as `20`. In this scenario, we do not have to bother about the `count` argument as the trajectory is clearly separable from the rest of the trajectories. Choosing the mean observation to replace these outlier points, the `outlierDetect` function generate the result shown in Figure \ref{fig:figs3}. 

```{r figs3, echo=TRUE, fig.width=8,fig.height=4,fig.align="center", fig.cap="\\label{fig:figs3}, Replacing outliers with mean observation"}

imp_traj_New <- outlierDetect(imp_traj, id_field = TRUE, method = 2, 
                              threshold = 20, count = 1, replace_with = 2)

imp_traj_New_long <- melt(imp_traj_New, id="location_ids") 

#plot function
p <-  ggplot(imp_traj_New_long, aes(x=variable, y=value,
            group=location_ids, color=location_ids)) + 
            geom_point() + 
            geom_line()
print(p)
```
The above plot function generate the plot shown in Figure \ref{fig:figs3}

***Note***: In a study that requires converting data from one measure to another, the user decides at what stage to remove the outliers (if any) from the data. For example, in [@Adepeju2019] in which crime `counts` is converted to `rates` and then to `proportion`, outliers were removed just before the `proportion` measurue is calculated.

###(v) 'Other' functions
Please see the `akmedoids` user manual for other useful `data manipulation` functions. 

#2. Data Clustering

Table \ref{tab:table2} shows the three key functions under this category. These include the `akmedoids.clust`for clustering trajectories according to the similarities of their long-term trends, the `statPrint` for generating both the descriptive statistics and the 'performance' plots of the best clustering solution, and lastly, the `spatialPlt` for representing the clusters spatially.

The long-term trends of trajectories can be defined in terms of a set of $n^{th}$-order polynomial functions or a combination of multiple sets of different $n^{th}$-order functions [@Nagin2005]. Currently, the `akmedoids` provides means of clustering trajectories based on the similarities of their linear trends ($1^{st}-order$), resulting into three simplified categories of trajectory groupings, namely; the `rising`, `stable`, and `falling` groups. The key benefit of this implementation is that it allows the user to ignore the short-term fluctuations of the trajectories, and focus on their long-term linear trends. A potential application of this clustering approach is in crime concentration research for identifying theory-based long-term stable groupings of trajectories in line with the socio-disorganisation and routine activities theories [@Griffith2004]. @Adepeju2019 were the first to demonstrate the utility of this idea for measuring long-term inequalities in the exposure to crime at micro-area levels, based on the conceptual (`inequality`) framework shown in Figure \ref{fig:figs4}. The linear trendline of each trajectory is approximated by a time-dependent regression line and map the resulting groupings on the original trajectories. 

```{r figs4, echo=FALSE, fig.cap=paste("Long-time linear trends of relative (`proportion`, `p`) crime exposure. Three inequality trends: trajectory i1: crime exposure is falling faster, i2, crime exposure is falling at the same rate, and i3, crime exposure is falling slower or increasing, relatively to the citywide trend. (Source:", "Adepeju et al. 2019)", sep=" "), out.width = '80%', fig.align="center"} 
knitr::include_graphics("inequality.png")
```
Asides clustering the trajectory trend lines, there are two major distinctions between the `akmedoids` and other existing longitudinal clustering techniques [@Genolini2010; @Nagin2005]. First, the `akmedoids` uses k-number of median trend lines of equal partitioned trend slopes at the initialisation stage in order to represent `‘anchors’` for the algorithm to begin. The purpose behind this initial step is to give the algorithm a theoretically-driven starting point and try and ensure that heterogenous trends end up in different clusters (@Khan2004; @Steinley2007). Second, instead of recomputing centroids based on the mean distances between each trajectory trend lines and the cluster center, the median of each cluster is selected then used as the next centroid. This then becomes the new anchor for the current iteration of the expectation-maximisation step [@Celeux1992]. This strategy is implemented in order to ensure robustness to trend lines representing outliers trajectories. The iteration is then continue until an objective function is maximised.  


```{r, echo=FALSE, include=FALSE}
require(knitr)
library(flextable)
library(kableExtra)
col1 <- c("1", "2","3")
col2 <- c("`akmedoids.clust`","`statPrint`","`spatialPlt`")
col3 <- c("`Anchored k-medoids clustering`","`Descriptive (Change) statistics and plots`","`Spatial plot of groups`")
col4 <- c("Clusters trajectories into a `k` number of groups according to the similarities in their long-term trend and determines the best solution based on the Calinski-Harabatz criterion","Generates the descriptive and change statistics of groups, and also plots the groups performances", "Plots the spatial manifestations of the groups")
tble2 <- data.frame(col1, col2, col3, col4)
tble2 <- tble2
```

```{r table2, results='asis', echo=FALSE, tidy.opts=list(width.cutoff=50)}
knitr::kable(tble2, caption = "`Data clustering` functions", col.names = c("SN","Function","Title","Description")) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "8em", background = "white") %>%
  column_spec(3, width = "12em", background = "white") %>%
  column_spec(4, width = "16em", background = "white")#%>%
  #row_spec(3:5, bold = T, color = "white", background = "#D7261E")
```

In the following sections, we provide a work example of clustering with `akmedoids.clust` function using the `prop_crime_per200_people` data plotted in Figure \ref{fig:figs5}. The `statPrint` function will then be used generate the descriptive summary of the clusters and `spatialPlt` function to generate the spatial manifestation of each group. 

```{r, echo=TRUE, include=TRUE}
#A quick check that sum of each column of proportion measures adds up to 1.  
colSums(prop_crime_per200_people[,2:ncol(prop_crime_per200_people)])
```

```{r figs5, echo=TRUE, fig.width=8,fig.height=4,fig.align="center", fig.cap="\\label{fig:figs5} Trajectory of crime proportions over time"}

#Visualising the proportion data

#preview and prepare data for plotting
head(prop_crime_per200_people)

prop_crime_per200_people_melt <- melt(prop_crime_per200_people, id="location_ids") 

#plot
p <-  ggplot(prop_crime_per200_people_melt, aes(x=variable, y=value,
            group=location_ids, color=location_ids)) + 
            geom_point() + 
            geom_line()
print(p)
```
The above plot function generate the plot shown in Figure \ref{fig:figs5}


### (i) `akmedoids.clust` function

***Data***: 
We use the `prop_crime_per200_people` object derived with the ['props'](#props) function. Each trajectory in Figure \ref{fig:figs5} represents the proportion of crimes per 200 residents in each location over time as calculated under the ['props'](#props) function. In other words, they represent the inequality trajectories and the goal is to extract the inequality trend lines (\ref{fig:figs4}) and cluster them accordingly. In order to achieve this task, a user is required to supply the trajectory object into the `akmedoids.clust` function and set `k`, a vector of length two specifying the minimum and maximum numbers of clusters to loop through. The `akmedoids.clust` function employs the `Calinki-Harabatz` score to determine the best cluster solution (@Calinski1974). The function is ran as follows:

```{r, echo=TRUE, include=TRUE}
#clustering
cluster_output <- akmedoids.clust(prop_crime_per200_people, id_field = TRUE, 
                                  method = "linear", k = c(3,8))

#print cluster solution
cluster_output
```

```{r figs6, echo=FALSE, fig.cap=paste("Clustering performance at different values of k"), out.width = '120%', fig.align="center"} 
knitr::include_graphics("caliHara.png")
```

The function generates a performance plot (Figure \ref{fig:figs6}) that shows the `Calinki-Harabatz` scores at different values of `k`. From the plot, the value of `k` at which the score is highest is considered the best. In this case, k = 5 is determine the best solution. The print out from the function also list the group membership (labels) associated to each trajectory, based on the best solution. This can be extracted as follows:   

```{r, echo=TRUE, include=TRUE}

#vector of group memberships
as.vector(cluster_output$optimSolution) 
```

The above extract represents the vector of group memberships of trajectories with indexes corresponding to the row number of the trajectory object (`prop_crime_per200_people`). 


### (ii) `statPrint` function:

Given the vector of group membership (labels), that is `as.vector(cluster_output$optimSolution)` and its corresponding trajectory object  `prop_crime_per200_people`, the `statPrint` function reports both the `descriptive` and the `change` statistics of the groups. The function also generates the plots of the `group memberships` and `performances` (i.e. share of measure captured) over time. An important argument of `statPrint` function is the `bandw` parameter which determines the final classification of the resulting groups into `Rising`, `Stable`, or `Falling`. Please, see the package user manual for more details about this parameter. The outputs can be generated for our current example as follows:


```{r, echo=TRUE, include=TRUE}

#cluster solution
clustr <- as.vector(cluster_output$optimSolution) 

#plotting the group membership
print(statPrint(clustr, prop_crime_per200_people, id_field=TRUE, 
                bandw = 0.40, type="lines", y.scaling="fixed"))

```

```{r figs7, echo=FALSE, fig.cap=paste("group memberships"), out.width = '120%', fig.align="center"} 
knitr::include_graphics("traj_perfm.png")
```

See Table \ref{tab:table3} for the detailed description of the field names of outputs above. These outputs are generated along with the plot of group memberships as shown in Figure \ref{fig:figs7}. By changing the argument `type` to `"stacked"` (i.e. `type="stacked"`), a `performance plot` is generated instead (see Figure \ref{fig:figs8}) Note that these plots draw from the `ggplot2` library [@Wickham2016]. For a more customised visualisation, we recommend that users deploy the `ggplot2` library directly. 

In the context of long-term inequality study, the combination of these outputs should allow inferences to be made regarding whether a group of areas have gained or lost in relation to the overall citywide trend [@Adepeju2019]. For example, whilst relative crime exposure have declined in 33.3% (groups `A` and `B`) of the area, the relative crime exposure have risen in 44.4% (groups `D` and `E`) of the area. The relative crime exposure can be said to be stable in 22.2% (group `C`) of the area, based on the `bandw` parameter.   


```{r figs8, echo=FALSE, fig.cap=paste("group performance over time"), out.width = '100%', fig.align="center"} 
knitr::include_graphics("traj_perfm2.png")
```

```{r, echo=FALSE, include=FALSE}

col1 <- c("1", "2","3","4","5","6", "7","8","9","10")
col2 <- c("`group`", "`n`", "`n(%)`", "`%Prop.time1`", "`%Prop.timeT`", "`Change`", "`%Change`", "`%+ve Traj.`", "`%-ve Traj.`", "`class`")
col3 <- c("`group membershp`", "`size (no.of.trajectories.)`", "`% size`", "`% proportion of obs. at time 1 (2001)`", "`proportion of obs. at time T (2009)`", "`absolute change in proportion between time1 and timeT`", "`% change in proportion between time 1 and time T`", "`% of trajectories with positive slopes`", "`% of trajectories with negative slopes`", "`classification based on slope`")
tble3 <- data.frame(col1, col2, col3)
tble3 <- tble3
```

```{r table3, results='asis', echo=FALSE, tidy.opts=list(width.cutoff=50)}
knitr::kable(tble3, caption = "`field description of clustering outputs`", col.names = c("SN","field","Description")) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "8em", background = "white") %>%
  column_spec(3, width = "12em", background = "white") #%>%
  #row_spec(3:5, bold = T, color = "white", background = "#D7261E")
```

### (ii) `spatialPlt` function:

In a study where the trajectories represent geographically-related observations, an analyst may be interested in examining whether there are spatial patterning in the observed phenomenon. Therefore, the resulting cluster groups have to be linked to their respective geographical locations in order to map their potential spatial patterning. In the example used in this study, each trajectory is the simulated longitidinal crime exposure of eleven fictitious census units created in a GIS environment (as `.shp` format). Using both our clustering output (`clustr`) and the `.shp`, we employ the `spatialPlt` function to visualise the spatial patterning of our clustering result. 

```{r, echo=FALSE, include=TRUE}
#import libraries
library(rgdal)

#read and visualise the shapefile
#readOS

#identifying the `id` field of the shapefile
#head(cityA@data)

#the `id` field is identified as "loc_code"
```

From the above, the `id` field of the shapefile is identified as `"loc_code"`. The `spatialPlt` function is then executed as follows:

```{r, echo=FALSE, include=TRUE}


```

Figure * represents the spatial plot of our longitudinal clusering analysis. For a more customised spatial plotting, please see the `ggplot2` package 


#Conclusion

The `akmedoids` package is developed in order to aid the replication of crime inequality investigation under a citywide crime drop scenario [@Adepeju2019]. However, many functions in this package, such as the `data manipulation` functions are applicable to any longitudinal datasets. For example, the `dataImputation` and `outlierDetect` functions which are used for imputing missing data and identifying outliers, respectively, should be applicable to any longitudinal data analysis. 

This package is being updated on a regular basis to add more functionalities to the existing `functions`. For instance, we understand that there are several other options for estimating missing data points, all these alternatives will be included in the package in due time. We also plan to add more `data manipulation` functions as well as expand the clustering `anchor` options for higher-order polynomials. 

Lastly, we employ users to report any bugs encountered while using the package and direct their queries to us. We also welcome contributions from users.

#References
